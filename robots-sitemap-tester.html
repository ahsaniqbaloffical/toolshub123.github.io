<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Robots.txt & Sitemap Tester | Toolshub123</title>
  <meta name="description" content="Free Robots.txt & Sitemap Tester tool to validate your site's crawlability and sitemap health.">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="canonical" href="https://toolshub123.com/robots-sitemap-tester.html" />

  <!-- Import Syne font -->
  <link href="https://fonts.googleapis.com/css2?family=Syne:wght@400;600;700&display=swap" rel="stylesheet">

  <!-- Universal CSS -->
  <link rel="stylesheet" href="toolshub123.css">

  <!-- Page-specific CSS will go in toolshub123.css using these unique classes -->
</head>
<body>
  <!-- Header (unchanged) -->
  <header>
    <img src="logo.png" alt="Toolshub123 Logo" class="logo">
    <nav class="header-nav">
      <a href="https://toolshub123.com/">Home</a>
      <a href="https://toolshub123.com/SEO%20tools.html" class="active">SEO Tools</a>
      <a href="https://toolshub123.com/finance%20tools.html">Finance Tools</a>
      <a href="https://toolshub123.com/utilitytools.html">Utility Tools</a>
      <a href="https://toolshub123.com/health%20tools.html">Health Tools</a>
    </nav>
  </header>

  <!-- Hero -->
  <section id="rs-tester-hero" class="rs-tester-hero">
    <h2 class="rs-tester-hero-title">Robots.txt & Sitemap Tester</h2>
    <p class="rs-tester-hero-subtitle">Paste your robots.txt or sitemap.xml content below or fetch directly via URL.</p>
  </section>
  <!-- ROBOTS & SITEMAP TOOL -->

<!-- URL method -->
<div id="rs-tester-url" class="rs-tester-container">
  <input type="text"
         id="rs-tester-inputUrl"
         class="rs-tester-input"
         placeholder="Enter robots.txt or sitemap.xml URL...">
  <div class="rs-tester-actions">
    <button class="btn-primary" onclick="rsTesterFetchAndAnalyze()">Fetch & Analyze</button>
  </div>
</div>

<!-- Manual paste method -->
<div id="rs-tester-manual" class="rs-tester-container">
  <textarea id="rs-tester-inputText"
            class="rs-tester-textarea"
            placeholder="Paste robots.txt or sitemap.xml here..."></textarea>
  <div class="rs-tester-actions">
    <button class="btn-primary" onclick="rsTesterAnalyzeInput()">Analyze</button>
    <button class="btn-secondary" onclick="rsTesterDownloadCSV()">Download Report (CSV)</button>
    <button class="btn-tertiary" onclick="rsTesterCopySummary()">Copy Summary</button>
  </div>

  <div id="rs-tester-results" class="rs-tester-results">
    <div id="rs-tester-summary" class="rs-tester-summary"></div>
    <table id="rs-tester-resultsTable" class="rs-tester-table">
      <thead>
        <tr>
          <th>Entry</th>
          <th>Type</th>
          <th>Status</th>
        </tr>
      </thead>
      <tbody></tbody>
    </table>
  </div>
</div>

<script>
async function rsTesterFetchAndAnalyze(){
  const url = document.getElementById("rs-tester-inputUrl").value.trim();
  if(!url){ alert("Enter a valid URL."); return; }
  try{
    const res = await fetch(url);
    if(!res.ok){ alert("Failed to fetch file. Check the URL."); return; }
    const text = await res.text();
    document.getElementById("rs-tester-inputText").value = text; 
    rsTesterAnalyzeInput(); 
  }catch(err){
    alert("Error fetching file: " + err.message);
  }
}

function rsTesterAnalyzeInput(){
  const text = document.getElementById("rs-tester-inputText").value.trim();
  if(!text){ alert("Paste robots.txt or sitemap.xml content."); return; }
  let rows = [];
  let summary = "";
  let metrics = {};
  let errorsList = [];
  let warningsList = [];

  if(text.startsWith("<")){ 
    // sitemap XML mode
    const xml = new DOMParser().parseFromString(text,"application/xml");
    const locs = [...xml.getElementsByTagName("loc")];
    const lastmods = [...xml.getElementsByTagName("lastmod")];
    const urls = locs.length;
    let errors = 0, warnings = 0, valid = 0;

    if(urls === 0){
      errors++;
      rows.push(["No <loc> entries","Sitemap","❌","error"]);
      errorsList.push({msg:"Sitemap contains no <loc> entries.",fix:"Add valid <loc> tags for each page URL."});
    } 
    locs.forEach((loc,i)=>{
      const url = loc.textContent.trim();
      if(url){
        valid++;
        rows.push([url,"URL","Valid","valid"]);
      } else {
        errors++;
        rows.push(["Empty <loc>","URL","❌","error"]);
        errorsList.push({msg:"Empty <loc> tag found in sitemap.",fix:"Ensure each <url> has a valid <loc> with a full URL."});
      }
      if(!lastmods[i]){
        warnings++;
        warningsList.push({msg:`URL ${url} has no <lastmod> tag.`,fix:"Add <lastmod> to indicate when the page was last updated."});
      }
    });

    const score = rsTesterSitemapScore(valid,errors,warnings,urls);
    summary = `[Sitemap] Compliance Score: ${score}`;
    metrics = {urls,valid,errors,warnings,score};

  } else { 
    // robots.txt mode
    const lines = text.split(/\r?\n/);
    let errors = 0, warnings = 0, valid = 0, directives = 0;
    lines.forEach(line=>{
      const l = line.trim(); if(!l) return;
      const parts = l.split(":");
      if(parts.length < 2){
        warnings++;
        rows.push([l,"Directive","Unknown","warning"]);
        warningsList.push({msg:`Line "${l}" is not a valid directive format.`,fix:"Use proper syntax: Directive: value (e.g., Disallow: /path)."});
        return;
      }
      const key = parts[0].toLowerCase();
      directives++;
      if(["user-agent","disallow","allow","sitemap"].includes(key)){
        valid++;
        rows.push([l,"Directive","Valid","valid"]);
      } else {
        warnings++;
        rows.push([l,"Directive","Unknown","warning"]);
        warningsList.push({msg:`Directive "${key}" is not recognized.`,fix:"Use only supported directives: User-agent, Disallow, Allow, Sitemap."});
      }
    });

    const score = rsTesterRobotsScore(valid,errors,warnings,directives);
    summary = `[Robots.txt] Compliance Score: ${score}`;
    metrics = {directives,valid,errors,warnings,score};
  }

  // Show summary
  document.getElementById("rs-tester-summary").innerHTML = "<h3>"+summary+"</h3>";

  // Metrics table
  let metricsHTML = "<table class='metrics-table'><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody>";
  for(const key in metrics){
    metricsHTML += `<tr><td>${key.charAt(0).toUpperCase()+key.slice(1)}</td><td>${metrics[key]}</td></tr>`;
  }
  metricsHTML += "</tbody></table>";

  // Detailed errors and warnings with fixes
  if(errorsList.length > 0){
    metricsHTML += "<h4 class='error-title'>Errors:</h4><ul class='error-list'>";
    errorsList.forEach(e=>{
      metricsHTML += `<li class='error-item'>${e.msg}<br><em>Fix: ${e.fix}</em></li>`;
    });
    metricsHTML += "</ul>";
  }
  if(warningsList.length > 0){
    metricsHTML += "<h4 class='warning-title'>Warnings:</h4><ul class='warning-list'>";
    warningsList.forEach(w=>{
      metricsHTML += `<li class='warning-item'>${w.msg}<br><em>Fix: ${w.fix}</em></li>`;
    });
    metricsHTML += "</ul>";
  }

  document.getElementById("rs-tester-summary").innerHTML += "<br>"+metricsHTML;

  // Detailed rows table
  const tbody = document.querySelector("#rs-tester-resultsTable tbody");
  tbody.innerHTML = "";
  rows.forEach(r=>{
    const tr = document.createElement("tr");
    tr.innerHTML = `<td>${r[0]}</td><td>${r[1]}</td><td><span class="badge ${r[3]}">${r[2]}</span></td>`;
    tbody.appendChild(tr);
  });
  document.getElementById("rs-tester-results").style.display = "block";
}

function rsTesterRobotsScore(valid,errors,warnings,directives){
  let score = 100 - errors*10 - warnings*5;
  if(valid === 0) score -= 20;
  if(score < 0) score = 0;
  return score;
}

function rsTesterSitemapScore(valid,errors,warnings,urls){
  let score = 100 - errors*10 - warnings*3;
  if(valid === 0) score -= 30;
  if(urls === 0) score = 0;
  if(score < 0) score = 0;
  return score;
}

/* Optional: implement these with unique names if needed
function rsTesterDownloadCSV(){ ... }
function rsTesterCopySummary(){ ... }
*/
</script>


  <!-- Features -->
<section id="rs-tester-features" class="rs-tester-section rs-tester-features">
  <h2 class="rs-tester-heading">Key Features of the Robots.txt & Sitemap Tester</h2>
  <ul class="rs-tester-list">
    <li class="rs-tester-list-item">✔ Quickly test and validate your <strong>robots.txt</strong> and <strong>sitemap.xml</strong> files for SEO compliance.</li>
    <li class="rs-tester-list-item">✔ Automatically detect whether the file is a robots.txt or sitemap and analyze its directives or URLs.</li>
    <li class="rs-tester-list-item">✔ Generate a clear <strong>SEO compliance score</strong> starting at 100, with deductions for errors and warnings.</li>
    <li class="rs-tester-list-item">✔ View a detailed summary and results table with easy‑to‑read status badges for each directive or URL.</li>
    <li class="rs-tester-list-item">✔ Export results to CSV or copy the summary for quick sharing with your SEO team.</li>
  </ul>
</section>

<!-- FAQs -->
<section id="rs-tester-faqs" class="rs-tester-section rs-tester-faqs">
  <h2 class="rs-tester-heading">Frequently Asked Questions</h2>
  <ul class="rs-tester-list">
    <li class="rs-tester-list-item"><strong>Do I need both robots.txt and a sitemap?</strong> Yes. The <strong>robots.txt file</strong> controls crawler access to your site, while the <strong>sitemap.xml</strong> helps search engines discover and index your content more efficiently.</li>
    
    <li class="rs-tester-list-item"><strong>Can I have multiple sitemaps?</strong> Absolutely. You can use a sitemap index file and reference multiple “Sitemap:” lines in your robots.txt to organize large websites.</li>
    
    <li class="rs-tester-list-item"><strong>Why should I avoid blocking CSS or JavaScript?</strong> Modern search engines render pages like browsers. Blocking CSS or JS in robots.txt can prevent proper rendering and negatively impact rankings.</li>
    
    <li class="rs-tester-list-item"><strong>Where should I place my robots.txt file?</strong> The robots.txt file must be located in the root directory of your domain (e.g., <code>https://example.com/robots.txt</code>) for search engines to detect it.</li>
    
    <li class="rs-tester-list-item"><strong>How often should I update my sitemap?</strong> Update your sitemap whenever new content is published or old content is removed. This ensures search engines always have the latest URLs to crawl.</li>
    
    <li class="rs-tester-list-item"><strong>Does robots.txt affect SEO rankings?</strong> Indirectly, yes. While robots.txt doesn’t change rankings itself, blocking important pages or resources can harm crawlability and indexing, which impacts SEO performance.</li>
    
    <li class="rs-tester-list-item"><strong>Can I use robots.txt to block duplicate content?</strong> Robots.txt can prevent crawlers from accessing duplicate pages, but for SEO it’s better to use canonical tags or noindex directives to handle duplicates properly.</li>
    
    <li class="rs-tester-list-item"><strong>What happens if my sitemap has errors?</strong> Search engines may ignore invalid entries or fail to index your site correctly. Using a sitemap tester helps identify and fix issues before they affect visibility.</li>
    
    <li class="rs-tester-list-item"><strong>Do all search engines follow robots.txt rules?</strong> Major search engines like Google, Bing, and Yahoo respect robots.txt directives, but some smaller crawlers or bots may ignore them.</li>
    
    <li class="rs-tester-list-item"><strong>Should I include images and videos in my sitemap?</strong> Yes. Adding image and video sitemaps helps search engines index rich media content, improving visibility in Google Images and video search results.</li>
  </ul>
</section>


<!-- About -->
<section id="rs-tester-about" class="rs-tester-section rs-tester-about">
  <h2 class="rs-tester-heading">About the Robots.txt & Sitemap Tester</h2>
  <p class="rs-tester-paragraph">
    The <strong>Robots.txt & Sitemap Tester</strong> is a free SEO tool built to help webmasters, digital marketers, and developers validate two of the most important files for search engine optimization. These files guide Google, Bing, and other search engines on how to crawl and index your website.
  </p>
  <p class="rs-tester-paragraph">With this tool, you can:</p>
  <ul class="rs-tester-list">
    <li class="rs-tester-list-item">✔ Detect errors and warnings in your <strong>robots.txt directives</strong> to ensure proper crawler access.</li>
    <li class="rs-tester-list-item">✔ Verify that your <strong>sitemap.xml</strong> contains valid, accessible URLs for indexing.</li>
    <li class="rs-tester-list-item">✔ Instantly calculate a compliance score to measure the overall health of your crawl settings.</li>
    <li class="rs-tester-list-item">✔ Export reports to CSV for audits or share summaries with your SEO team.</li>
  </ul>
  <p class="rs-tester-paragraph">
    By simplifying technical SEO checks, this tool ensures your site remains crawlable, indexable, and optimized for better visibility in search results.
  </p>
</section>


  <!-- Footer (unchanged) -->
  <footer>
    <p>&copy; 2026 Toolshub123 | Built with ❤️ by Ahsan</p>
    <p>
      <a href="https://toolshub123.com/PrivacyPolicy.html">Privacy Policy</a> | 
      <a href="https://toolshub123.com/terms%20of%20service.html">Terms of Service</a> | 
      <a href="https://toolshub123.com/about.html">About</a> | 
      <a href="https://toolshub123.com/contact.html">Contact Us</a>
    </p>
  </footer>
</body>
</html>


